{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21238,"status":"ok","timestamp":1653316728078,"user":{"displayName":"Rodrigo Castellano","userId":"09621897243893675794"},"user_tz":-120},"id":"pTFLf1AR8JsF","outputId":"eefb09a9-2b2c-4150-8ecc-b049170b02c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab_Notebooks/DL\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd  /content/drive/MyDrive/Colab_Notebooks/DL"]},{"cell_type":"markdown","metadata":{"id":"myWo28hTINuC"},"source":["#Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fmq6l8zx8b7W"},"outputs":[],"source":["!pip install transformers==3.5.1\n","!pip install torch==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUhUXOzv8Mau"},"outputs":[],"source":["import random\n","import time\n","import datetime\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.metrics import f1_score, accuracy_score\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup,Adafactor \n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","source":["seed_val = 5\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"metadata":{"id":"fEBK93-Bp9Mr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HK7Q9mt3C4wi"},"source":["# Load dataa nd tokenize"]},{"cell_type":"markdown","metadata":{"id":"_iTUgWl2WQtP"},"source":["Steps:\n","\n","- Tokenize all the text with BertTokenizer (check max len of tweets)\n","\n","- Get tokens and masks from BertTokenizer. Put the data into Tensor dataset (input_ids, attention_masks, labels)\n","\n","- Divide in train and validation. Create a DataLoader for each set\n","\n","- Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":349,"status":"ok","timestamp":1653318979286,"user":{"displayName":"Rodrigo Castellano","userId":"09621897243893675794"},"user_tz":-120},"id":"oCa9JoMmD2_a","outputId":"40341b9e-877d-4f18-db6a-863ac4a23a6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of labelled tweets: 7613\n","\n"]},{"output_type":"display_data","data":{"text/plain":["         id         keyword location  \\\n","1014   1473  body%20bagging      NaN   \n","7565  10814         wrecked      NaN   \n","4406   6263       hijacking      NaN   \n","\n","                                                   text  target  \n","1014  ÛÏ@MacDaddy_Leo: ?????? No Caption Needed ??....       1  \n","7565   Wrecked tired but not gonna be asleep before 3??       0  \n","4406  The ship has arrived safely. So it was quite u...       0  "],"text/html":["\n","  <div id=\"df-1c192769-faee-4c98-9ff3-80592f91bc83\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1014</th>\n","      <td>1473</td>\n","      <td>body%20bagging</td>\n","      <td>NaN</td>\n","      <td>ÛÏ@MacDaddy_Leo: ?????? No Caption Needed ??....</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7565</th>\n","      <td>10814</td>\n","      <td>wrecked</td>\n","      <td>NaN</td>\n","      <td>Wrecked tired but not gonna be asleep before 3??</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4406</th>\n","      <td>6263</td>\n","      <td>hijacking</td>\n","      <td>NaN</td>\n","      <td>The ship has arrived safely. So it was quite u...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c192769-faee-4c98-9ff3-80592f91bc83')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1c192769-faee-4c98-9ff3-80592f91bc83 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1c192769-faee-4c98-9ff3-80592f91bc83');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["data = pd.read_csv('Dataset/train.csv')\n","pred = pd.read_csv('Dataset/test.csv')\n","print(f'Number of labelled tweets: {data.shape[0]}\\n')\n","display(data.sample(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bvw0D9KTECkZ"},"outputs":[],"source":["# Get the text from the df and the index to later divide it between train and test\n","labels = data['target'].values\n","sequences = data.text.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyLHvK8IENOv"},"outputs":[],"source":["# Get tokens using BertTokenizer. It does a mapping from the words to their IDs and \n","# does padding or truncating depending on len_max. A mask is added, with 1's or 0's to\n","# distinguish between [PAD] tokens and the rest of the tokens.  \n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n","\n","print('Original: ', sequences[12])\n","# Tokenizes sequences\n","print('Tokenized: ', tokenizer.tokenize(sequences[12]))\n","# ID's for each sentence\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequences[0])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zf1OthbfHk2I"},"outputs":[],"source":["idx = int(sequences.shape[0]*0.9)\n","train= sequences[:idx]\n","test = sequences[idx:]\n","print(data.shape)\n","print(test.shape)\n","\n","labels_training = labels[:idx]\n","labels_testing = labels[idx:]\n","print(labels_training.shape)\n","print(labels_testing.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCTD8c7gHr_P"},"outputs":[],"source":["def tokenize(sequences,labels):\n","\n","    ids,masks = [],[]\n","    for sequence in sequences:\n","        encoded = tokenizer.encode_plus(sequence,add_special_tokens = True,\\\n","                       truncation='longest_first',max_length = 84,pad_to_max_length = True, \\\n","                       return_attention_mask = True,return_tensors = 'pt')\n","        \n","        # Put it in the list of id's\n","        ids.append(encoded['input_ids'])\n","        # Same for the attention mask\n","        masks.append(encoded['attention_mask'])\n","\n","    # Convert it to pytorch tensors\n","    ids = torch.cat(ids, dim=0)\n","    masks = torch.cat(masks, dim=0)\n","\n","    labels = torch.tensor(labels)\n","    print(labels.shape)\n","    return ids,masks,labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vihO2e8BHwC0"},"outputs":[],"source":["# Tokenize the sentences and get their IDs \n","train_ids, train_masks, labels_training = tokenize_map(train, labels_training)\n","test_ids, test_masks, labels_test= tokenize_map(test, labels_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lv-iAwFtH1eJ"},"outputs":[],"source":["dataset = TensorDataset(input_ids, attention_masks, labels_training)\n","\n","# split in 80% training, 20% val\n","train_size = int(train.shape[0]*0.8)\n","val_size = idx - train_size\n","\n","# Randomize it\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(train_size,'training samples')\n","print(val_size,'validation samples')\n","\n","# For batch size it's better 16 or 32\n","batch_size = 32\n","\n","# Obtain dataloaders, for training random and for validation sequentially (order doesnt matter)\n","train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset),\\  \n","                    batch_size = batch_size)\n","\n","validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),\\ \n","                                   batch_size = batch_size)\n","\n","prediction_data = TensorDataset(test_input_ids, test_attention_masks)\n","test_dataloader = DataLoader(prediction_data, sampler=SequentialSampler(prediction_data), batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"1NtRnHrwJCtj"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1653318977900,"user":{"displayName":"Rodrigo Castellano","userId":"09621897243893675794"},"user_tz":-120},"id":"EXbGAJAhDkw4","outputId":"c1b0da70-3a19-485a-c865-d05d4b62c8cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of GPUs: 1 , Name:  Tesla T4\n"]}],"source":["# Check if there's GPU\n","\n","if torch.cuda.is_available():    \n","    device = torch.device('cuda')    \n","    print('Number of GPUs:',torch.cuda.device_count(),', Name: ',torch.cuda.get_device_name(0))\n","else:\n","    print('Using CPU')\n","    device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLoL8ebpICpV"},"outputs":[],"source":["model = BertForSequenceClassification.from_pretrained('bert-large-uncased',num_labels = 2,\\\n","            output_attentions = False, output_hidden_states = False)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p89rHgh3Imc-"},"outputs":[],"source":["# https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n","\n","# optimizer = AdamW(model.parameters(),lr = 1e-4, eps = 1e-8)\n","\n","# Replace AdamW with Adafactor\n","optimizer = Adafactor(\n","    model.parameters(),\n","    lr=1e-3,\n","    eps=(1e-30, 1e-3),\n","    clip_threshold=1.0,\n","    decay_rate=-0.8,\n","    beta1=None,\n","    weight_decay=0.0,\n","    relative_step=False,\n","    scale_parameter=False,\n","    warmup_init=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ovjlnm91I6aL"},"outputs":[],"source":["epochs = 3\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6gR3FPdJdRs"},"outputs":[],"source":["def get_acc(preds, labels):\n","    return accuracy_score(labels.flatten() ,  np.argmax(preds, axis=1).flatten())\n","\n","def get_f1_score(preds, labels):      \n","    return f1_score(labels.flatten() , np.argmax(preds, axis=1).flatten())\n","\n","def get_time(time):    \n","    return str(datetime.timedelta(seconds=int(round((time)))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6Ie-Nofj3u_"},"outputs":[],"source":["# Based on https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","# and https://www.kaggle.com/code/datafan07/disaster-tweets-nlp-eda-bert-with-transformers\n","\n","training_stats = []\n","total_t0 = time.time()\n","\n","for epoch_i in range(0, epochs):\n","    \n","    # Training\n","    print('\\nEpoch {:}/{:}'.format(epoch_i + 1, epochs))\n","\n","    t0 = time.time()\n","    total_train_loss = 0    \n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","\n","        if step % 50 == 0 and not step == 0:\n","            elapsed = get_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device).to(torch.int64)\n","        b_input_mask = batch[1].to(device).to(torch.int64)\n","        b_labels = batch[2].to(device).to(torch.int64)\n","\n","        model.zero_grad()        \n","\n","        # Forward         \n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        \n","        total_train_loss += loss.item()\n","        \n","        # Backward\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","\n","    training_time = get_time(time.time() - t0)\n","    print('\\nAverage training loss: {0:.2f}'.format(avg_train_loss))\n","    print('Training epcoh took: {:}'.format(training_time))\n","        \n","    # Validation\n","    print ('validation: ')\n","    t0 = time.time()\n","\n","    model.eval()\n","\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    total_eval_f1 = 0\n","    nb_eval_steps = 0\n","\n","    for batch in validation_dataloader:\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","             \n","        with torch.no_grad():        \n","            (loss, logits) = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,\\\n","                                   labels=b_labels,return_dict=False)\n","            \n","        total_eval_loss += loss.item()\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        total_eval_accuracy += get_acc(logits, label_ids)\n","        total_eval_f1 += get_f1_score(logits, label_ids)\n","        \n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n","  \n","    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n","    print('  F1: {0:.2f}'.format(avg_val_f1))\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    validation_time = get_time(time.time() - t0)\n","    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n","    print('  Validation took: {:}'.format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(   {  'epoch': epoch_i + 1,\n","                                'Training Loss': avg_train_loss,\n","                                'Valid. Loss': avg_val_loss,\n","                                'Valid. Accur.': avg_val_accuracy,\n","                                'Val_F1' : avg_val_f1,\n","                                'Training Time': training_time,\n","                                'Validation Time': validation_time   } )\n","print('\\nDone!')\n","print('Total training took {:} (h:mm:ss)'.format(get_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4S1TX_SrVwm"},"outputs":[],"source":["# Evaluation\n","model.eval()\n","predictions = []\n","\n","for batch in test_dataloader:\n","\n","  batch = tuple(t.to(device) for t in batch)\n","  b_ids, b_mask, = batch\n","  \n","  with torch.no_grad():\n","      outputs = model(b_ids, token_type_ids=None, \n","                      attention_mask=b_mask)\n","\n","  logits = outputs[0]\n","  logits = logits.detach().cpu().numpy()\n","  predictions.append(logits)"]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"AsEAXfiSnAiN"}},{"cell_type":"code","source":["pd.set_option('precision', 3)\n","results = pd.DataFrame(data=training_stats)\n","results = results.set_index('epoch')\n","results"],"metadata":{"id":"Wd5xZK2Smqac"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUkhtWsgrZmU"},"outputs":[],"source":["# Get the predictions\n","preds = [element for prediction in predictions for element in prediction]\n","preds = np.argmax(preds, axis=1).flatten()"]},{"cell_type":"code","source":["df_stats['test_acc'] = len(np.where(flat_predictions==labels_testing.numpy())[0])/len(flat_predictions)\n","df_stats['opt,lr'] ='Adafactor,1e-3'\n","df_stats"],"metadata":{"id":"gRU4aVedFZiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model\n","# model.save_pretrained(\"model/local-model-checkpoint_2\")\n","\n","# save the info\n","df_stats.to_csv('model/local-model-checkpoint_5/df_stats.csv', index=True, header=True)"],"metadata":{"id":"LJyKlntb4fUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The code was inspired by https://www.kaggle.com/code/datafan07/disaster-tweets-nlp-eda-bert-with-transformers"],"metadata":{"id":"afZsb8Njn6pK"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["HK7Q9mt3C4wi","1NtRnHrwJCtj","AsEAXfiSnAiN"],"name":"Bert","provenance":[],"authorship_tag":"ABX9TyOPXabmuE1eZ7Tvx/aWokyl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}