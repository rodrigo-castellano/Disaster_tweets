{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f18197",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d74ccbc0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871c4fa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1196237c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf537f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training dataframe visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d6417e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         id keyword location  \\\n0         1     NaN      NaN   \n1         4     NaN      NaN   \n2         5     NaN      NaN   \n3         6     NaN      NaN   \n4         7     NaN      NaN   \n...     ...     ...      ...   \n7608  10869     NaN      NaN   \n7609  10870     NaN      NaN   \n7610  10871     NaN      NaN   \n7611  10872     NaN      NaN   \n7612  10873     NaN      NaN   \n\n                                                   text  target  \\\n0     Our Deeds are the Reason of this #earthquake M...       1   \n1                Forest fire near La Ronge Sask. Canada       1   \n2     All residents asked to 'shelter in place' are ...       1   \n3     13,000 people receive #wildfires evacuation or...       1   \n4     Just got sent this photo from Ruby #Alaska as ...       1   \n...                                                 ...     ...   \n7608  Two giant cranes holding a bridge collapse int...       1   \n7609  @aria_ahrary @TheTawniest The out of control w...       1   \n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n7611  Police investigating after an e-bike collided ...       1   \n7612  The Latest: More Homes Razed by Northern Calif...       1   \n\n                                                 tokens  \\\n0            [deed, reason, earthquake, allah, forgive]   \n1                   [forest, fire, ronge, sask, canada]   \n2     [resident, asked, shelter, place, notified, of...   \n3     [people, receive, wildfire, evacuation, order,...   \n4     [got, sent, photo, ruby, alaska, smoke, wildfi...   \n...                                                 ...   \n7608  [giant, crane, holding, bridge, collapse, near...   \n7609  [control, wild, fire, california, northern, pa...   \n7610                             [utc, volcano, hawaii]   \n7611  [police, investigating, bike, collided, car, l...   \n7612  [latest, home, razed, northern, california, wi...   \n\n                                            clean_tweet  \n0                  deed reason earthquake allah forgive  \n1                         forest fire ronge sask canada  \n2     resident asked shelter place notified officer ...  \n3     people receive wildfire evacuation order calif...  \n4     got sent photo ruby alaska smoke wildfire pour...  \n...                                                 ...  \n7608    giant crane holding bridge collapse nearby home  \n7609  control wild fire california northern part sta...  \n7610                                 utc volcano hawaii  \n7611  police investigating bike collided car little ...  \n7612  latest home razed northern california wildfire...  \n\n[7613 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>tokens</th>\n      <th>clean_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>[deed, reason, earthquake, allah, forgive]</td>\n      <td>deed reason earthquake allah forgive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>[forest, fire, ronge, sask, canada]</td>\n      <td>forest fire ronge sask canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>[resident, asked, shelter, place, notified, of...</td>\n      <td>resident asked shelter place notified officer ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>[people, receive, wildfire, evacuation, order,...</td>\n      <td>people receive wildfire evacuation order calif...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n      <td>[giant, crane, holding, bridge, collapse, near...</td>\n      <td>giant crane holding bridge collapse nearby home</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>10870</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n      <td>[control, wild, fire, california, northern, pa...</td>\n      <td>control wild fire california northern part sta...</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>10871</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n      <td>[utc, volcano, hawaii]</td>\n      <td>utc volcano hawaii</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>10872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n      <td>[police, investigating, bike, collided, car, l...</td>\n      <td>police investigating bike collided car little ...</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n      <td>[latest, home, razed, northern, california, wi...</td>\n      <td>latest home razed northern california wildfire...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944be41d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Tweets class repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9113ea53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the training set, there are 4342 tweets non-disaster related, and 3271 tweets disaster related.\n"
     ]
    }
   ],
   "source": [
    "class_0 = 0\n",
    "class_1 = 0\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    if train.target[k] == 0:\n",
    "        class_0 += 1\n",
    "    else:\n",
    "        class_1 += 1\n",
    "\n",
    "print('In the training set, there are '+str(class_0)+' tweets non-disaster related, and '+str(class_1)+' tweets disaster related.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1172dc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data preproccesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ee684",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a4654d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a lemmatizer to fold inflected word forms together.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download( 'stopwords', quiet=True )\n",
    "nltk.download( 'wordnet', quiet=True )\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV\n",
    "           }\n",
    "\n",
    "# Get part of speech for a word. \n",
    "def get_wordnet_pos( word ):\n",
    "    tag = nltk.pos_tag( [word] )[0][1][0].upper()\n",
    "    return tag_dict.get( tag , wordnet.NOUN )\n",
    "\n",
    "# Remove web links from a tweet.\n",
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "# Remove retweet and @user information from a tweet.\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    \n",
    "    # remove retweet\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    \n",
    "    # remove tweeted at\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)     \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67f965",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define stopwords and ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5dbdec2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the default English stopwords list from nltk.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "f = open('data/stopwords.txt') ; my_stopwords = set(f.read().split('\\n')) ; f.close()\n",
    "stopwords = list(set(stopwords).union(my_stopwords))\n",
    "\n",
    "# Get a list of punctuation to clean from tweet.\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@#'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d84098",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Tokenization and lemmatization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78d7bbb7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenization_process(tweet, bigrams=False, remove_hashtag=False):\n",
    "    tweet = remove_users(tweet)  # remove users (handles)\n",
    "    tweet = remove_links(tweet)  # remove web links\n",
    "    tweet = tweet.lower()        # convert tweet to lower case\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "\n",
    "    # tokenize the tweet.\n",
    "    tweet_token_list = [word for word in tweet.strip().split(' ')\n",
    "                            if len(word)>2 ]\n",
    "\n",
    "    # lemmatize the words in the tweet.\n",
    "    tweet_token_list = [lemmatizer.lemmatize(word)\n",
    "        for word in tweet_token_list]\n",
    "\n",
    "    # remove stop words from lemma list.\n",
    "    tweet_token_list = [word for word in tweet_token_list\n",
    "                       if word not in stopwords]\n",
    "\n",
    "    # deal with bigrams if requested.\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    \n",
    "    # remove hashtags\n",
    "    if remove_hashtag:\n",
    "        for token in tweet_token_list:\n",
    "            token.replace('#', '')\n",
    "                \n",
    "    return tweet_token_list\n",
    "\n",
    "def to_list(tweet_token_list):\n",
    "    \n",
    "    # join the processed words in the tweet back to a string with a blank separating the words.\n",
    "    tweet = ' '.join(list(tweet_token_list))\n",
    "\n",
    "    # return the cleaned tweet.\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce60ba0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train['tokens'] = train.text.apply(tokenization_process)\n",
    "train['clean_tweet'] = train.tokens.apply(to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "0                    deed reason earthquake allah forgive\n1                           forest fire ronge sask canada\n2       resident asked shelter place notified officer ...\n3       people receive wildfire evacuation order calif...\n4       got sent photo ruby alaska smoke wildfire pour...\n                              ...                        \n7608      giant crane holding bridge collapse nearby home\n7609    control wild fire california northern part sta...\n7610                                   utc volcano hawaii\n7611    police investigating bike collided car little ...\n7612    latest home razed northern california wildfire...\nName: clean_tweet, Length: 7613, dtype: object"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['clean_tweet']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "e5b4d69c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "with help of https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9823a32b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_raw = train[['target']].to_numpy(dtype=int)\n",
    "y = np.zeros((train.shape[0], 2))\n",
    "for k in range(y_raw.shape[0]):\n",
    "    y[k, y_raw[k]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word = values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "9c7df465",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "MAX_LEN=max(len(tokens)for tokens in train['tokens'])\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(train['tokens'])\n",
    "sequences=tokenizer_obj.texts_to_sequences(train['tokens'])\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "num_words=len(tokenizer_obj.word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(tokenizer_obj.word_index.items()):\n",
    "    if i < num_words:\n",
    "        emb_vec=embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(Bidirectional(LSTM(25, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-4)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweet_pad, y_raw, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=16,epochs=20,validation_data=(X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}